{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP FINAL BIF DATA\n",
    "\n",
    "Entrega final del trabajo práctico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando e instalando dependencias...\n",
      "\n",
      "✓ pandas ya está instalado\n",
      "✓ numpy ya está instalado\n",
      "✓ matplotlib ya está instalado\n",
      "✓ seaborn ya está instalado\n",
      "✓ requests ya está instalado\n",
      "✓ requests_cache ya está instalado\n",
      "\n",
      "✓ Todas las dependencias están listas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bloque 1: Configuración y Carga\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, mode, mean\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Ruta al archivo (Asegúrate de subirlo a Databricks mediante 'File -> Upload Data')\n",
    "# Una vez subido, Databricks te dará la ruta, suele ser algo como:\n",
    "file_path = \"/FileStore/tables/heart_disease.csv\" \n",
    "\n",
    "# Cargar CSV\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "print(\"Datos originales:\")\n",
    "df.show(5)\n",
    "\n",
    "# Bloque 2: Limpieza y Preprocesamiento (Data Engineering)\n",
    "# 1. Tratar valores nulos (He detectado nulos en 'ca' y 'thal' en tu archivo)\n",
    "# Vamos a rellenarlos con la moda (el valor más común) o un valor fijo.\n",
    "# Para simplificar en este TP, usaremos -1 para indicar que faltaba el dato, o la moda.\n",
    "# Aquí rellenamos con la media para 'ca' y la moda para 'thal' (simplificado para Spark):\n",
    "df_clean = df.na.fill({'ca': 0.0, 'thal': 3.0}) # Asumimos valores comunes\n",
    "\n",
    "# 2. Transformar la variable objetivo 'num'\n",
    "# En este dataset, 0 es sano, y 1,2,3,4 son distintos grados de enfermedad.\n",
    "# Para el TP, conviene hacerlo binario: 0 = No Enfermo, 1 = Enfermo.\n",
    "df_final = df_clean.withColumn(\"label\", when(col(\"num\") > 0, 1).otherwise(0))\n",
    "\n",
    "# 3. Separar Features Numéricas y Categóricas\n",
    "# Esto es clave para el pipeline de ML posterior\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n",
    "\n",
    "# (Opcional) Puedes castear categóricas a String si quieres usar OneHotEncoder después\n",
    "# Por ahora las dejamos numéricas para un modelo simple (Random Forest las maneja bien)\n",
    "\n",
    "# Bloque 3: Split y Guardado\n",
    "# Dividimos en Train (80%) y Test (20%)\n",
    "train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Guardar en formato DELTA (Mejor práctica en Databricks)\n",
    "train_data.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"heart_train\")\n",
    "test_data.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"heart_test\")\n",
    "\n",
    "print(f\"Datos de entrenamiento guardados: {train_data.count()} registros.\")\n",
    "print(f\"Datos de test guardados: {test_data.count()} registros.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
